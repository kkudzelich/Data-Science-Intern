![Data Science Intern at Data Glacier](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/077be7e3-44ed-4ee5-9543-8f0b3dab9333)

![Week 1](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
## Week 1 (30 May - 06 June)
![ML](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
### Version Control Assignment 
Clone the VC repo [(Link)](https://github.com/DataGlacier/VC.git), Create a new branch, Checkout newly created branch, Run the add.py and provide my name and fav sport as input, Run the test script using command:   pytest test/test.py -s, ignore warning and if there is no error then add, commit and push your changes to repo create pull request and assign to reviewer.

Link: https://github.com/kkudzelich/Data-Science-Intern/tree/main/Week%201

![Week 2](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
## Week 2 (07 June - 14 June)
![ML](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)

### Project: G2M insight for Cab Investment firm
The Client XYZ is a private firm in US. Due to remarkable growth in the Cab Industry in last few years and multiple key players in the market, it is planning for        an investment in Cab industry and as per their Go-to-Market(G2M) strategy they want to understand the market before taking final decision.

Datasets contain information on 2 cab companies. Each file (data set) provided represents different aspects of the customer profile. XYZ is interested in using your actionable insights to help them identify the right company to make their investment.

<b>Tasks</b>
* Identify relationships across the files
* Exploratory Data Analysis(EDA)
* Multiple hypothesis and investigate

Link: https://github.com/kkudzelich/Data-Science-Intern/tree/main/Week%202

![Week 3](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
## Week 3 (15 June - 21 June)
![ML](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)

### Project: G2M insight for Cab Investment firm
Continuing work on Project "G2M insight for Cab Investment firm":
* Development of recommendations for investing in the Cab industry.
* Presentation of the completed work.

Link: https://github.com/kkudzelich/Data-Science-Intern/tree/main/Week%203

![Week 4](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
## Week 4 (22 June - 28 June)
![ML](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)

### Deployment on Flask
* Using the data from the previous weeks, I trained a Machine Learning model (XGBoost) to predict the Profit per Trip, taking into account various features (like "City", "Payment Mode", "Gender", "KM Travelled", "Date", etc.).
* Then I deployed this model (created an API) using the __Flask framework__. This API allows you to use forecasting capabilities through HTTP requests.

You can read about the deployment process and see several examples of using the model in production in the file ['Flask_Deployment_Document.pdf'](https://github.com/kkudzelich/Data-Science-Intern/blob/main/Week%204/Flask_Deployment_Document.pdf).

Link: https://github.com/kkudzelich/Data-Science-Intern/tree/main/Week%204

![Week 5](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
## Week 5 (29 June - 05 July)
![ML](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)

### Heroku Cloud Deployment
* Using the data and trained XGBoost model from the previous weeks, I performed Cloud Deployment using __Heroku__.

You can read about the cloud deployment steps in the file ['Heroku_Cloud_Deployment.pdf'](https://github.com/kkudzelich/Data-Science-Intern/blob/main/Week%205/Heroku_Cloud_Deployment.pdf).

Link: https://github.com/kkudzelich/Data-Science-Intern/tree/main/Week%205

![Week 6](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)
## Week 6 (06 July - 12 July)
![ML](https://github.com/kkudzelich/Data-Science-Intern/assets/107845717/fe88e464-292b-4ede-b195-a7eae6d4aa2e)

### File Ingestion & Schema Validation
* Firstly, I took large size of data (2GB+) and applied different methods of reading like Dask, Modin, ray and Pandas to check the computational efficiency.
* After that, I performed basic validation on the data columns and then validated the number of columns and column names in the ingested file using YAML.
* Finally, I compressed the csv file into gz format and obtained a summary of the file.

Link: https://github.com/kkudzelich/Data-Science-Intern/tree/main/Week%206
